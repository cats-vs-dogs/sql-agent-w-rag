{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b8470c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT\n",
    "\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional, Sequence, TypedDict, Union\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# --- LangChain / LangGraph core ---\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.documents import Document as LCDocument\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from openai import AzureOpenAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "from langgraph.graph import END, StateGraph, MessagesState\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# --- SQL and validation ---\n",
    "import sqlglot\n",
    "from sqlglot import parse_one\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "# --- Utilities ---\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ddfe848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "OPENAI_API_KEY=os.getenv('AZURE_OPENAI_API_KEY_US')\n",
    "OPENAI_API_KEY_E=os.getenv('AZURE_OPENAI_API_KEY_US2')\n",
    "\n",
    "# os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "os.environ['OPENAI_API_VERSION'] = '2024-08-01-preview'\n",
    "os.environ['AZURE_OPENAI_ENDPOINT'] = 'https://azure-chat-try-2.openai.azure.com/'\n",
    "\n",
    "os.environ['OPENAI_API_VERSION_E'] = '2024-12-01-preview'\n",
    "os.environ['AZURE_OPENAI_ENDPOINT_E'] = 'https://agents-4on.openai.azure.com/'\n",
    "\n",
    "# LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
    "# os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "# os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "# os.environ['LANGCHAIN_PROJECT'] = \"rag-sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f7e9c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = AzureOpenAI(\n",
    "    api_key = OPENAI_API_KEY_E,  \n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT_E\"),\n",
    "    api_version=os.getenv(\"OPENAI_API_VERSION_E\")\n",
    ")\n",
    "deployment=\"text-embedding-3-small-eus2\"\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key = OPENAI_API_KEY,  \n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_deployment=\"chat-endpoint-us-gpt4o\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56702259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_embedding(text: str, model=deployment, **kwargs) -> List[float]:\n",
    "#     # replace newlines, which can negatively affect performance.\n",
    "#     text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "#     response = emb_model.embeddings.create(input=[text], model=model, **kwargs)\n",
    "\n",
    "#     return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f48e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Database\n",
    "# from langchain_community.utilities import SQLDatabase\n",
    "# db = SQLDatabase.from_uri(\"sqlite:///./database/credit-risk.db\", sample_rows_in_table_info=2)\n",
    "# print(db.dialect)\n",
    "# print(db.get_usable_table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce80257d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "\n",
    "DB_CONN_STR = os.getenv(\"DB_CONN_STR\", \"sqlite:///./database/credit-risk.db\")\n",
    "VECTOR_DIR = os.getenv(\"VECTOR_DIR\", \"./database\")\n",
    "DB_DOCS_DIR = os.getenv(\"DB_DOCS_DIR\", \"./database\")\n",
    "\n",
    "# Safety limits\n",
    "MAX_ROWS_DEFAULT = int(os.getenv(\"MAX_ROWS_DEFAULT\", \"2000\"))\n",
    "QUERY_TIMEOUT_SECS = int(os.getenv(\"QUERY_TIMEOUT_SECS\", \"60\"))\n",
    "REWRITE_MAX_ATTEMPTS = int(os.getenv(\"REWRITE_MAX_ATTEMPTS\", \"2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b67e64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BB033377\\AppData\\Local\\Temp\\ipykernel_30980\\3595163269.py:60: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n",
      "  vs = Chroma(embedding_function=emb_model, persist_directory=vdir)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AzureOpenAI' object has no attribute 'embed_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mensure_vector_store\u001b[39m\u001b[34m(vdir)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# quick sanity retrieve\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m _ = \u001b[43mvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m vs\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:350\u001b[39m, in \u001b[36mChroma.similarity_search\u001b[39m\u001b[34m(self, query, k, filter, **kwargs)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run similarity search with Chroma.\u001b[39;00m\n\u001b[32m    341\u001b[39m \n\u001b[32m    342\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m \u001b[33;03m    List[Document]: List of documents most similar to the query text.\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:439\u001b[39m, in \u001b[36mChroma.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, where_document, **kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     query_embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m(query)\n\u001b[32m    440\u001b[39m     results = \u001b[38;5;28mself\u001b[39m.__query_collection(\n\u001b[32m    441\u001b[39m         query_embeddings=[query_embedding],\n\u001b[32m    442\u001b[39m         n_results=k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    445\u001b[39m         **kwargs,\n\u001b[32m    446\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'AzureOpenAI' object has no attribute 'embed_query'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     66\u001b[39m         records = docs_to_text_chunks(DB_DOCS)\n\u001b[32m     67\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m build_vector_store(records, persist_dir=vdir)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m VECTORSTORE = \u001b[43mensure_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m RETRIEVER = VECTORSTORE.as_retriever(search_kwargs={\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m6\u001b[39m})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mensure_vector_store\u001b[39m\u001b[34m(vdir)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# rebuild if load fails\u001b[39;00m\n\u001b[32m     66\u001b[39m     records = docs_to_text_chunks(DB_DOCS)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvdir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mbuild_vector_store\u001b[39m\u001b[34m(records, persist_dir)\u001b[39m\n\u001b[32m     48\u001b[39m splitter = RecursiveCharacterTextSplitter(chunk_size=\u001b[32m1200\u001b[39m, chunk_overlap=\u001b[32m120\u001b[39m)\n\u001b[32m     49\u001b[39m chunks = splitter.split_documents(records)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m vs = \u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43memb_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m vs.persist()\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m vs\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:887\u001b[39m, in \u001b[36mChroma.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    885\u001b[39m texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    886\u001b[39m metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:843\u001b[39m, in \u001b[36mChroma.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[32m    838\u001b[39m         api=chroma_collection._client,\n\u001b[32m    839\u001b[39m         ids=ids,\n\u001b[32m    840\u001b[39m         metadatas=metadatas,\n\u001b[32m    841\u001b[39m         documents=texts,\n\u001b[32m    842\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m         \u001b[43mchroma_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    849\u001b[39m     chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m(texts)\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    279\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    280\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    281\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[31mAttributeError\u001b[39m: 'AzureOpenAI' object has no attribute 'embed_documents'"
     ]
    }
   ],
   "source": [
    "# 1) LOAD DB DOCS (JSON) AND PREP RAG INDEX\n",
    "\n",
    "def load_db_docs(dir_path: str) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load JSON documentation files from a folder.\n",
    "    Returns a dict keyed by table name -> metadata dict.\n",
    "    \"\"\"\n",
    "    docs = {}\n",
    "    for p in Path(dir_path).glob(\"*.json\"):\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "            d = json.load(f)\n",
    "            table = d[\"table\"]\n",
    "            docs[table] = d\n",
    "    return docs\n",
    "\n",
    "DB_DOCS = load_db_docs(DB_DOCS_DIR)\n",
    "\n",
    "def docs_to_text_chunks(db_docs: Dict[str, Dict[str, Any]]) -> List[LCDocument]:\n",
    "    \"\"\"\n",
    "    Flatten JSON table docs into LangChain Document chunks with helpful metadata.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for table, meta in db_docs.items():\n",
    "        # full text for table meta\n",
    "        full = json.dumps(meta, ensure_ascii=False, indent=2)\n",
    "        records.append(LCDocument(\n",
    "            page_content=f\"TABLE: {table}\\n{full}\",\n",
    "            metadata={\"table\": table, \"section\": \"full\"}\n",
    "        ))\n",
    "        # optionally split by columns for finer recall\n",
    "        for col in meta.get(\"columns\", []):\n",
    "            col_text = json.dumps(col, ensure_ascii=False, indent=2)\n",
    "            records.append(LCDocument(\n",
    "                page_content=f\"TABLE: {table}\\nCOLUMN: {col['name']}\\n{col_text}\",\n",
    "                metadata={\"table\": table, \"section\": \"column\", \"column\": col[\"name\"]}\n",
    "            ))\n",
    "        # relationships\n",
    "        for rel in meta.get(\"relationships\", []):\n",
    "            rel_text = json.dumps(rel, ensure_ascii=False, indent=2)\n",
    "            records.append(LCDocument(\n",
    "                page_content=f\"TABLE: {table}\\nRELATIONSHIP:\\n{rel_text}\",\n",
    "                metadata={\"table\": table, \"section\": \"relationship\"}\n",
    "            ))\n",
    "    return records\n",
    "\n",
    "\n",
    "def build_vector_store(records: List[LCDocument], persist_dir: str = VECTOR_DIR) -> Chroma:\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=120)\n",
    "    chunks = splitter.split_documents(records)\n",
    "    vs = Chroma.from_documents(chunks, embedding=emb_model, persist_directory=persist_dir)\n",
    "    vs.persist()\n",
    "    return vs\n",
    "\n",
    "def ensure_vector_store(vdir: str = VECTOR_DIR) -> Chroma:\n",
    "    # Create or load\n",
    "    if not Path(vdir).exists():\n",
    "        Path(vdir).mkdir(parents=True, exist_ok=True)\n",
    "    # Try to open\n",
    "    try:\n",
    "        vs = Chroma(embedding_function=emb_model, persist_directory=vdir)\n",
    "        # quick sanity retrieve\n",
    "        _ = vs.similarity_search(\"test\", k=1)\n",
    "        return vs\n",
    "    except Exception:\n",
    "        # rebuild if load fails\n",
    "        records = docs_to_text_chunks(DB_DOCS)\n",
    "        return build_vector_store(records, persist_dir=vdir)\n",
    "\n",
    "VECTORSTORE = ensure_vector_store()\n",
    "RETRIEVER = VECTORSTORE.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202d878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d246a9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acb16ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd954841",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DIR = os.getenv(\"VECTOR_DIR\", \"./vectorstore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745065bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c25c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c135dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03445c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d87add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAgentState(AgentState):\n",
    "    \"\"\"Extended agent state that includes task tracking and virtual file system.\n",
    "\n",
    "    Inherits from LangGraph's AgentState and adds:\n",
    "    - schema: The retrieved database schema\n",
    "    - sql_query: The query that refelcts the user question and the database schema\n",
    "    - query_valid: a validation of the query correctness\n",
    "    - query_result: the result of running the qyery against the database  \n",
    "    \"\"\"\n",
    "    schema: str\n",
    "    sql_query: str\n",
    "    query_valid: bool\n",
    "    \n",
    "    \n",
    "    # class GraphState(MessagesState):\n",
    "#     sql_query: str\n",
    "#     query_valid: bool\n",
    "#     query_result: str\n",
    "    query_result: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1962db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tool\n",
    "def get_schema_tool(input: None) -> str:\n",
    "    \"\"\"Retrieve the database schema\"\"\"\n",
    "    schema = db.get_table_info()\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed58182",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_instructions = \"\"\"\n",
    "You are a SQL expert with a strong attention to detail.\n",
    "\n",
    "Given an input question, create a syntactically correct SQLite query to run to help find the answer. \n",
    "\n",
    "When generating the query:\n",
    "\n",
    "Unless the user specifies in his question a specific number of examples they wish to obtain, limit your query to at most 3 results. \n",
    "For example, if the user asks for the top 5 results, you should NOT limit the query to 3 results.\n",
    "\n",
    "You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "\n",
    "Never query for all the columns from a specific table, only ask for a the few relevant columns given the question.\n",
    "\n",
    "Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "\n",
    "DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n",
    "\n",
    "Only use the following tables:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def generate_query_tool(\n",
    "    ,\n",
    "    state: Annotated[dict, GraphState]\n",
    "   ) -> str:\n",
    "    \"\"\"Write a syntactically valid SQL query.\n",
    "        Args:\n",
    "            query: Search query to execute\n",
    "            state: Injected agent state for file storage\n",
    "            tool_call_id: Injected tool call identifier\n",
    "            max_results: Maximum number of results to return (default: 1)\n",
    "            topic: Topic filter - 'general', 'news', or 'finance' (default: 'general')\n",
    "\n",
    "        Returns:\n",
    "            Command that saves full results to files and provides minimal summary\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    question = input[\"question\"]\n",
    "    schema = input[\"schema\"]\n",
    "    prompt = f\"Given the question: {question} and the schema: {schema}, write a correct SQL query.\"\n",
    "    response = await llm.ainvoke([HumanMessage(content=prompt)], RunnableConfig())\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e4540f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512038b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9b1975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed4a064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7d989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162629b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_schema_tool(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated\n",
    "\n",
    "write_query_instructions = \"\"\"\n",
    "You are a SQL expert with a strong attention to detail.\n",
    "\n",
    "Given an input question, create a syntactically correct SQLite query to run to help find the answer. \n",
    "\n",
    "When generating the query:\n",
    "\n",
    "Unless the user specifies in his question a specific number of examples they wish to obtain, limit your query to at most 3 results. \n",
    "For example, if the user asks for the top 5 results, you should NOT limit the query to 3 results.\n",
    "\n",
    "You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "\n",
    "Never query for all the columns from a specific table, only ask for a the few relevant columns given the question.\n",
    "\n",
    "Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "\n",
    "DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n",
    "\n",
    "Only use the following tables:\n",
    "{schema}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "class QueryOutput(TypedDict):\n",
    "    \"\"\"Generated SQL query.\"\"\"\n",
    "    query: Annotated[str, ..., \"Syntactically valid SQL query.\"]\n",
    "\n",
    "def write_query(state: State):\n",
    "    \"\"\"Generate SQL query to fetch information.\"\"\"\n",
    "\n",
    "    question = state[\"question\"]\n",
    "    schema = db.get_table_info()\n",
    "    \n",
    "    prompt = write_query_instructions.format(\n",
    "        question = question,\n",
    "        schema = schema        \n",
    "    ) \n",
    "\n",
    "    structured_llm = llm.with_structured_output(QueryOutput)\n",
    "    result = structured_llm.invoke(prompt)\n",
    "    # return {\"query\": result[\"query\"]}    \n",
    "    return {\"query\": \"SELECT \\n    SUM(EXPOSURE) AS total_off_balance_exposure \\nFROM \\n    transactions\\nWHERE \\n    DATE LIKE '2023-sep%' AND\\n    EXPOSURE_TYPE = 'OFF_BALANCE';\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tool\n",
    "def generate_sql_tool(question: str, docs: str) -> str:\n",
    "    \"\"\"Generate SQL query from question + docs.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an assistant that writes SQL queries.\n",
    "    User question: {question}\n",
    "    Database info: {docs}\n",
    "    \n",
    "    Write a SQL query that answers the question.\n",
    "    \"\"\"\n",
    "    return llm.invoke(prompt).content\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".agents-ud (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
