{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acbf6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import uuid\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY_E=os.getenv('AZURE_OPENAI_API_KEY_US2')\n",
    "os.environ['OPENAI_API_VERSION_E'] = '2024-12-01-preview'\n",
    "os.environ['AZURE_OPENAI_ENDPOINT_E'] = 'https://agents-4on.openai.azure.com/'\n",
    "os.environ['AZURE_OPENAI_EMBEDDING_DEPLOYMENT_E'] = \"text-embedding-3-large-eus2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57feb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"./database\"                  # Folder with .json files\n",
    "PERSIST_DIR = \"./database\"                # Chroma persistence path\n",
    "COLLECTION_NAME = \"json_embeddings\"       # Logical collection name\n",
    "RESET_COLLECTION = False                  # If True, clears existing data\n",
    "\n",
    "# Chunking (safe defaults for text-embedding-3-* models)\n",
    "TARGET_CHUNK_TOKENS = 800\n",
    "CHUNK_OVERLAP_TOKENS = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354fb240",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    api_key=OPENAI_API_KEY_E,\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT_E'),  \n",
    "    api_version=os.getenv('OPENAI_API_VERSION_E'),\n",
    "    azure_deployment=os.getenv('AZURE_OPENAI_EMBEDDING_DEPLOYMENT_E')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74feafe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flatten_json(obj: Any, prefix: str = \"\") -> List[str]:\n",
    "    \"\"\"Flatten any JSON to lines like 'path.to.key: value' for embedding.\"\"\"\n",
    "    lines: List[str] = []\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            new_prefix = f\"{prefix}.{k}\" if prefix else k\n",
    "            lines.extend(flatten_json(v, new_prefix))\n",
    "    elif isinstance(obj, list):\n",
    "        for i, v in enumerate(obj):\n",
    "            new_prefix = f\"{prefix}[{i}]\"\n",
    "            lines.extend(flatten_json(v, new_prefix))\n",
    "    else:\n",
    "        val = \"\" if obj is None else str(obj)\n",
    "        if prefix:\n",
    "            lines.append(f\"{prefix}: {val}\")\n",
    "        else:\n",
    "            lines.append(val)\n",
    "    return lines\n",
    "\n",
    "\n",
    "def json_to_text(obj: Any, file_name: str) -> str:\n",
    "    \"\"\"Turn JSON into a readable text block, optionally selecting fields.\"\"\"\n",
    "    if ONLY_FIELDS and isinstance(obj, dict):\n",
    "        # Only keep selected fields at top-level if present\n",
    "        subset: Dict[str, Any] = {k: obj.get(k) for k in ONLY_FIELDS if k in obj}\n",
    "        lines = flatten_json(subset, \"\")\n",
    "    else:\n",
    "        lines = flatten_json(obj, \"\")\n",
    "    header = f\"Source: {file_name}\\n\"\n",
    "    return header + \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def load_json_files(input_dir: str) -> List[Document]:\n",
    "    \"\"\"Load .json files and convert to LangChain Documents (with chunking).\"\"\"\n",
    "    paths = sorted(glob.glob(os.path.join(input_dir, \"*.json\")))\n",
    "    if not paths:\n",
    "        print(f\"[Info] No .json files found under: {input_dir}\")\n",
    "        return []\n",
    "\n",
    "    splitter = TokenTextSplitter(\n",
    "        encoding_name=\"cl100k_base\",\n",
    "        chunk_size=TARGET_CHUNK_TOKENS,\n",
    "        chunk_overlap=CHUNK_OVERLAP_TOKENS,\n",
    "    )\n",
    "\n",
    "    docs: List[Document] = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[Warn] Skipping {p}: {e}\")\n",
    "            continue\n",
    "\n",
    "        base_text = json_to_text(data, file_name=os.path.basename(p))\n",
    "        chunks = splitter.split_text(base_text)\n",
    "\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            doc = Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"source_file\": os.path.basename(p),\n",
    "                    \"chunk_index\": idx,\n",
    "                    \"total_chunks\": len(chunks),\n",
    "                },\n",
    "                # You can add id in metadata; Chroma will create its own id unless specified.\n",
    "            )\n",
    "            docs.append(doc)\n",
    "\n",
    "    print(f\"[Info] Prepared {len(docs)} chunk(s) from {len(paths)} file(s).\")\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f8b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INGEST INTO CHROMA\n",
    "\n",
    "def build_or_load_vectorstore(docs: List[Document]) -> Chroma:\n",
    "    \"\"\"\n",
    "    Create or load a persistent Chroma vector store (via LangChain).\n",
    "    If RESET_COLLECTION is True and store exists, it will be cleared by re-creating.\n",
    "    \"\"\"\n",
    "    if RESET_COLLECTION and os.path.exists(PERSIST_DIR):\n",
    "        # A blunt reset: remove the directory\n",
    "        import shutil\n",
    "        try:\n",
    "            shutil.rmtree(PERSIST_DIR)\n",
    "            print(f\"[Info] Removed existing Chroma store at {PERSIST_DIR}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Warn] Failed to remove {PERSIST_DIR}: {e}\")\n",
    "\n",
    "    # If docs provided (fresh ingest), use from_documents; else load existing store.\n",
    "    if docs:\n",
    "        vs = Chroma.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=embeddings,\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            persist_directory=PERSIST_DIR,\n",
    "        )\n",
    "        vs.persist()  # Ensure it is flushed to disk\n",
    "        print(f\"[Success] Ingested {len(docs)} chunks into Chroma collection '{COLLECTION_NAME}'.\")\n",
    "        return vs\n",
    "\n",
    "    # Fallback: load existing persistent store\n",
    "    vs = Chroma(\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        persist_directory=PERSIST_DIR,\n",
    "    )\n",
    "    print(f\"[Info] Loaded existing Chroma store from {PERSIST_DIR}\")\n",
    "    return vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b4a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE QUERY / RETRIEVAL\n",
    "\n",
    "def run_query(vs: Chroma, query: str, k: int = 4):\n",
    "    \"\"\"\n",
    "    Performs a similarity search using the same Azure embeddings.\n",
    "    Returns documents with metadata + distances (if needed).\n",
    "    \"\"\"\n",
    "    # LangChain Chroma: similarity_search returns top-k Documents\n",
    "    results = vs.similarity_search(query, k=k)\n",
    "\n",
    "    print(\"\\n=== Top Matches ===\")\n",
    "    for i, d in enumerate(results, start=1):\n",
    "        md = d.metadata or {}\n",
    "        src = md.get(\"source_file\", \"unknown\")\n",
    "        idx = md.get(\"chunk_index\", -1)\n",
    "        total = md.get(\"total_chunks\", -1)\n",
    "        print(f\"\\nRank #{i}\")\n",
    "        print(f\"Source: {src} (chunk {idx+1}/{total})\")\n",
    "        preview = d.page_content[:500].replace(\"\\n\", \" \")\n",
    "        if len(d.page_content) > 500:\n",
    "            preview += \" ...\"\n",
    "        print(preview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e782844",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_json_files(INPUT_DIR)\n",
    "vectorstore = build_or_load_vectorstore(docs)\n",
    "\n",
    "# Sample query (adjust or comment out)\n",
    "run_query(vectorstore, \"Which table could be joined with the 'transactions' table?\", k=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c90414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".agents-ud (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
